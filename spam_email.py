# -*- coding: utf-8 -*-
"""Spam Email.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O5oAf8lgtmCqQ-dNtNk9el64tRViLsnh
"""

import pandas as pd


file_path = "/content/spam.csv"
df = pd.read_csv(file_path, encoding='latin-1')

# Display the first few rows and column names to understand the structure
df.head(), df.columns

# Step 1: Clean the dataset
# Keep only the necessary columns and rename them
df_cleaned = df[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})

# Display basic information and a few samples
df_cleaned.info(), df_cleaned.head()

import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK data files (only needed once in your Colab)
nltk.download('punkt')
nltk.download('stopwords')


# Define stopwords and punctuation
stop_words = set(stopwords.words('english'))
punctuations = set(string.punctuation)

# Preprocessing function
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and punctuation
    cleaned_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    # Join tokens back to string
    return " ".join(cleaned_tokens)

# Apply preprocessing to the text column
df_cleaned['clean_text'] = df_cleaned['text'].apply(preprocess_text)

# Show sample of cleaned texts
df_cleaned[['text', 'clean_text']].head()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Apply TF-IDF on the cleaned text
X = vectorizer.fit_transform(df_cleaned['clean_text'])

# Encode labels: spam = 1, ham = 0
y = df_cleaned['label'].map({'ham': 0, 'spam': 1})

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the model
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\nðŸ“Š Classification Report:\n", classification_report(y_test, y_pred))
print("\nðŸ§® Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
labels = ['Ham', 'Spam']

# Plot using seaborn
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Spam Detection')
plt.show()

